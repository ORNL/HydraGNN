============================= test session starts ==============================
platform darwin -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0 -- /Users/7ml/Documents/Codes/HydraGNN/.venv/bin/python
cachedir: .pytest_cache
rootdir: /Users/7ml/Documents/Codes/HydraGNN
configfile: pytest.ini
collecting ... collected 68 items

tests/test_graphs.py::pytest_train_model[ci.json-SAGE] PASSED            [  1%]
tests/test_graphs.py::pytest_train_model[ci.json-GIN] PASSED             [  2%]
tests/test_graphs.py::pytest_train_model[ci.json-GAT] PASSED             [  4%]
tests/test_graphs.py::pytest_train_model[ci.json-MFC] PASSED             [  5%]
tests/test_graphs.py::pytest_train_model[ci.json-PNA] PASSED             [  7%]
tests/test_graphs.py::pytest_train_model[ci.json-PNAPlus] PASSED         [  8%]
tests/test_graphs.py::pytest_train_model[ci.json-CGCNN] PASSED           [ 10%]
tests/test_graphs.py::pytest_train_model[ci.json-SchNet] PASSED          [ 11%]
tests/test_graphs.py::pytest_train_model[ci.json-DimeNet] PASSED         [ 13%]
tests/test_graphs.py::pytest_train_model[ci.json-EGNN] PASSED            [ 14%]
tests/test_graphs.py::pytest_train_model[ci.json-PNAEq] PASSED           [ 16%]
tests/test_graphs.py::pytest_train_model[ci.json-PAINN] PASSED           [ 17%]
tests/test_graphs.py::pytest_train_model[ci.json-MACE] PASSED            [ 19%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-SAGE] PASSED  [ 20%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-GIN] PASSED   [ 22%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-GAT] PASSED   [ 23%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-MFC] PASSED   [ 25%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNA] PASSED   [ 26%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAPlus] PASSED [ 27%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-CGCNN] PASSED [ 29%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-SchNet] PASSED [ 30%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-DimeNet] PASSED [ 32%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-EGNN] PASSED  [ 33%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAEq] PASSED [ 35%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PAINN] PASSED [ 36%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-MACE] PASSED  [ 38%]
tests/test_graphs.py::pytest_train_model_lengths[GAT] PASSED             [ 39%]
tests/test_graphs.py::pytest_train_model_lengths[PNA] PASSED             [ 41%]
tests/test_graphs.py::pytest_train_model_lengths[PNAPlus] PASSED         [ 42%]
tests/test_graphs.py::pytest_train_model_lengths[CGCNN] PASSED           [ 44%]
tests/test_graphs.py::pytest_train_model_lengths[SchNet] PASSED          [ 45%]
tests/test_graphs.py::pytest_train_model_lengths[DimeNet] PASSED         [ 47%]
tests/test_graphs.py::pytest_train_model_lengths[EGNN] PASSED            [ 48%]
tests/test_graphs.py::pytest_train_model_lengths[PNAEq] PASSED           [ 50%]
tests/test_graphs.py::pytest_train_model_lengths[PAINN] PASSED           [ 51%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[GAT-multihead-GPS] PASSED [ 52%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNA-multihead-GPS] PASSED [ 54%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAPlus-multihead-GPS] PASSED [ 55%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[CGCNN-multihead-GPS] PASSED [ 57%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[SchNet-multihead-GPS] PASSED [ 58%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[DimeNet-multihead-GPS] PASSED [ 60%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[EGNN-multihead-GPS] PASSED [ 61%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAEq-multihead-GPS] PASSED [ 63%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PAINN-multihead-GPS] PASSED [ 64%]
tests/test_graphs.py::pytest_train_mace_model_lengths[MACE] PASSED       [ 66%]
tests/test_graphs.py::pytest_train_equivariant_model[EGNN] PASSED        [ 67%]
tests/test_graphs.py::pytest_train_equivariant_model[SchNet] PASSED      [ 69%]
tests/test_graphs.py::pytest_train_equivariant_model[PNAEq] PASSED       [ 70%]
tests/test_graphs.py::pytest_train_equivariant_model[PAINN] PASSED       [ 72%]
tests/test_graphs.py::pytest_train_equivariant_model[MACE] PASSED        [ 73%]
tests/test_graphs.py::pytest_train_model_vectoroutput[GAT] PASSED        [ 75%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNA] FAILED        [ 76%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNAPlus] PASSED    [ 77%]
tests/test_graphs.py::pytest_train_model_vectoroutput[SchNet] PASSED     [ 79%]
tests/test_graphs.py::pytest_train_model_vectoroutput[DimeNet] PASSED    [ 80%]
tests/test_graphs.py::pytest_train_model_vectoroutput[EGNN] PASSED       [ 82%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNAEq] PASSED      [ 83%]
tests/test_graphs.py::pytest_train_model_conv_head[SAGE] FAILED          [ 85%]
tests/test_graphs.py::pytest_train_model_conv_head[GIN] PASSED           [ 86%]
tests/test_graphs.py::pytest_train_model_conv_head[GAT] FAILED           [ 88%]
tests/test_graphs.py::pytest_train_model_conv_head[MFC] FAILED           [ 89%]
tests/test_graphs.py::pytest_train_model_conv_head[PNA] FAILED           [ 91%]
tests/test_graphs.py::pytest_train_model_conv_head[PNAPlus] FAILED       [ 92%]
tests/test_graphs.py::pytest_train_model_conv_head[SchNet] FAILED        [ 94%]
tests/test_graphs.py::pytest_train_model_conv_head[DimeNet] FAILED       [ 95%]
tests/test_graphs.py::pytest_train_model_conv_head[EGNN] FAILED          [ 97%]
tests/test_graphs.py::pytest_train_model_conv_head[PNAEq] FAILED         [ 98%]
tests/test_graphs.py::pytest_train_model_conv_head[PAINN] FAILED         [100%]

=================================== FAILURES ===================================
_____________________ pytest_train_model_vectoroutput[PNA] _____________________

mpnn_type = 'PNA', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mpnn_type = 'PNA', global_attn_engine = None, global_attn_type = None
ci_input = 'ci_vectoroutput.json', use_lengths = True, overwrite_data = False
use_deepspeed = False, overwrite_config = None

    def unittest_train_model(
        mpnn_type,
        global_attn_engine,
        global_attn_type,
        ci_input,
        use_lengths,
        overwrite_data=False,
        use_deepspeed=False,
        overwrite_config=None,
    ):
        world_size, rank = hydragnn.utils.distributed.get_comm_size_and_rank()
    
        os.environ["SERIALIZED_DATA_PATH"] = os.getcwd()
    
        # Read in config settings and override model type.
        config_file = os.path.join(os.getcwd(), "tests/inputs", ci_input)
        with open(config_file, "r") as f:
            config = json.load(f)
        config["NeuralNetwork"]["Architecture"]["global_attn_engine"] = global_attn_engine
        config["NeuralNetwork"]["Architecture"]["global_attn_type"] = global_attn_type
        config["NeuralNetwork"]["Architecture"]["mpnn_type"] = mpnn_type
    
        # Overwrite config settings if provided
        if overwrite_config:
            config = merge_config(config, overwrite_config)
    
        """
        to test this locally, set ci.json as
        "Dataset": {
           ...
           "path": {
                   "train": "serialized_dataset/unit_test_singlehead_train.pkl",
                   "test": "serialized_dataset/unit_test_singlehead_test.pkl",
                   "validate": "serialized_dataset/unit_test_singlehead_validate.pkl"}
           ...
        """
        # use pkl files if exist by default
        for dataset_name in config["Dataset"]["path"].keys():
            if dataset_name == "total":
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + ".pkl"
                )
            else:
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + "_"
                    + dataset_name
                    + ".pkl"
                )
            if os.path.exists(pkl_file):
                config["Dataset"]["path"][dataset_name] = pkl_file
    
        # In the unit test runs, it is found MFC favors graph-level features over node-level features, compared with other models;
        # hence here we decrease the loss weight coefficient for graph-level head in MFC.
        if mpnn_type == "MFC" and ci_input == "ci_multihead.json":
            config["NeuralNetwork"]["Architecture"]["task_weights"][0] = 2
    
        # Only run with edge lengths for models that support them.
        if use_lengths:
            config["NeuralNetwork"]["Architecture"]["edge_features"] = ["lengths"]
    
        if rank == 0:
            num_samples_tot = 500
            # check if serialized pickle files or folders for raw files provided
            pkl_input = False
            if list(config["Dataset"]["path"].values())[0].endswith(".pkl"):
                pkl_input = True
            # only generate new datasets, if not pkl
            if not pkl_input:
                for dataset_name, data_path in config["Dataset"]["path"].items():
                    if overwrite_data:
                        shutil.rmtree(data_path)
                    if not os.path.exists(data_path):
                        os.makedirs(data_path)
                    if dataset_name == "total":
                        num_samples = num_samples_tot
                    elif dataset_name == "train":
                        num_samples = int(
                            num_samples_tot
                            * config["NeuralNetwork"]["Training"]["perc_train"]
                        )
                    elif dataset_name == "test":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    elif dataset_name == "validate":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    if not os.listdir(data_path):
                        tests.deterministic_graph_data(
                            data_path, number_configurations=num_samples
                        )
        MPI.COMM_WORLD.Barrier()
    
        # Since the config file uses PNA already, test the file overload here.
        # All the other models need to use the locally modified dictionary.
        if mpnn_type == "PNA" and not use_lengths:
            hydragnn.run_training(config_file, use_deepspeed)
        else:
            hydragnn.run_training(config, use_deepspeed)
    
        (
            error,
            error_mse_task,
            true_values,
            predicted_values,
        ) = hydragnn.run_prediction(config, use_deepspeed)
    
        # Set RMSE and sample MAE error thresholds
        thresholds = {
            "SAGE": [0.20, 0.20],
            "PNA": [0.20, 0.20],
            "PNAPlus": [0.20, 0.20],
            "MFC": [0.20, 0.30],
            "GIN": [0.25, 0.20],
            "GAT": [0.60, 0.70],
            "CGCNN": [0.50, 0.40],
            "SchNet": [0.20, 0.20],
            "DimeNet": [0.50, 0.50],
            "EGNN": [0.20, 0.20],
            "PNAEq": [0.60, 0.60],
            "PAINN": [0.60, 0.60],
            "MACE": [0.60, 0.70],
        }
        if use_lengths and ("vector" not in ci_input):
            thresholds["CGCNN"] = [0.175, 0.175]
            thresholds["PNA"] = [0.10, 0.10]
            thresholds["PNAPlus"] = [0.10, 0.10]
        if use_lengths and "vector" in ci_input:
            thresholds["PNA"] = [0.2, 0.15]
            thresholds["PNAPlus"] = [0.2, 0.15]
        if ci_input == "ci_conv_head.json":
            thresholds["GIN"] = [0.26, 0.51]
            thresholds["SchNet"] = [0.30, 0.30]
    
        verbosity = 2
    
        for ihead in range(len(true_values)):
            error_head_mse = error_mse_task[ihead]
            error_str = (
                str("{:.6f}".format(error_head_mse)) + " < " + str(thresholds[mpnn_type][0])
            )
            hydragnn.utils.print.print_distributed(verbosity, "head: " + error_str)
            assert (
                error_head_mse < thresholds[mpnn_type][0]
            ), "Head RMSE checking failed for " + str(ihead)
    
            head_true = true_values[ihead]
            head_pred = predicted_values[ihead]
            # Check individual samples
            mae = torch.nn.L1Loss()
            sample_mean_abs_error = mae(head_true, head_pred)
            error_str = (
                "{:.6f}".format(sample_mean_abs_error)
                + " < "
                + str(thresholds[mpnn_type][1])
            )
>           assert (
                sample_mean_abs_error < thresholds[mpnn_type][1]
            ), f"MAE sample checking failed! MAE: {sample_mean_abs_error:.6f} >= threshold: {thresholds[mpnn_type][1]} for model: {mpnn_type}"
E           AssertionError: MAE sample checking failed! MAE: 0.152753 >= threshold: 0.15 for model: PNA
E           assert tensor(0.1528) < 0.15

tests/test_graphs.py:192: AssertionError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
----------------------------- Captured stderr call -----------------------------
Degree max:   0%|          | 0/354 [00:00<?, ?it/s]Degree max: 100%|██████████| 354/354 [00:00<00:00, 126289.33it/s]
Degree bincount:   0%|          | 0/354 [00:00<?, ?it/s]Degree bincount: 100%|██████████| 354/354 [00:00<00:00, 104327.12it/s]
Degree max:   0%|          | 0/354 [00:00<?, ?it/s]Degree max: 100%|██████████| 354/354 [00:00<00:00, 126752.91it/s]
Degree bincount:   0%|          | 0/354 [00:00<?, ?it/s]Degree bincount: 100%|██████████| 354/354 [00:00<00:00, 103570.29it/s]
0: Load existing model: ./logs/PNA-r-2.0-ncl-2-hd-8-ne-80-lr-0.01-bs-16-data-unit_test_multihead-node_ft-1-task_weights-1.0-1.0-1.0-1.0-1.0-/PNA-r-2.0-ncl-2-hd-8-ne-80-lr-0.01-bs-16-data-unit_test_multihead-node_ft-1-task_weights-1.0-1.0-1.0-1.0-1.0-.pk
0: head: 0.001288 < 0.2
0: head: 0.008708 < 0.2
0: head: 0.040381 < 0.2
______________________ pytest_train_model_conv_head[SAGE] ______________________

mpnn_type = 'SAGE', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:97: in _
    model = get_distributed_model(
hydragnn/utils/distributed/distributed.py:379: in get_distributed_model
    model = DDP(model, **ddp_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:802: in __init__
    self._log_and_throw(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DistributedDataParallel(
  (module): SAGEStack(
    (graph_convs): ModuleList(
      (0): Sequential(
        (0) - SA...=True, track_running_stats=True)
      )
    )
    (activation_function): ReLU()
    (graph_shared): ModuleDict()
  )
)
err_type = <class 'RuntimeError'>
err_msg = "Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules"

    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f"{str(err_type)}: {err_msg}")
>       raise err_type(err_msg)
E       RuntimeError: Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules

.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:1143: RuntimeError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
______________________ pytest_train_model_conv_head[GAT] _______________________

mpnn_type = 'GAT', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:97: in _
    model = get_distributed_model(
hydragnn/utils/distributed/distributed.py:379: in get_distributed_model
    model = DDP(model, **ddp_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:802: in __init__
    self._log_and_throw(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DistributedDataParallel(
  (module): GATStack(
    (graph_convs): ModuleList(
      (0): Sequential(
        (0) - GAT...=True)
      )
    )
    (activation_function): ReLU()
    (out_lin): Identity()
    (graph_shared): ModuleDict()
  )
)
err_type = <class 'RuntimeError'>
err_msg = "Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules"

    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f"{str(err_type)}: {err_msg}")
>       raise err_type(err_msg)
E       RuntimeError: Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules

.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:1143: RuntimeError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
______________________ pytest_train_model_conv_head[MFC] _______________________

mpnn_type = 'MFC', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:97: in _
    model = get_distributed_model(
hydragnn/utils/distributed/distributed.py:379: in get_distributed_model
    model = DDP(model, **ddp_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:802: in __init__
    self._log_and_throw(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DistributedDataParallel(
  (module): MFCStack(
    (graph_convs): ModuleList(
      (0): Sequential(
        (0) - MFC...=True, track_running_stats=True)
      )
    )
    (activation_function): ReLU()
    (graph_shared): ModuleDict()
  )
)
err_type = <class 'RuntimeError'>
err_msg = "Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules"

    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f"{str(err_type)}: {err_msg}")
>       raise err_type(err_msg)
E       RuntimeError: Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules

.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:1143: RuntimeError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
______________________ pytest_train_model_conv_head[PNA] _______________________

mpnn_type = 'PNA', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:132: in unittest_train_model
    hydragnn.run_training(config_file, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:62: in _
    run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:97: in _
    model = get_distributed_model(
hydragnn/utils/distributed/distributed.py:379: in get_distributed_model
    model = DDP(model, **ddp_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:802: in __init__
    self._log_and_throw(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DistributedDataParallel(
  (module): PNAStack(
    (graph_convs): ModuleList(
      (0): Sequential(
        (0) - PNA...=True, track_running_stats=True)
      )
    )
    (activation_function): ReLU()
    (graph_shared): ModuleDict()
  )
)
err_type = <class 'RuntimeError'>
err_msg = "Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules"

    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f"{str(err_type)}: {err_msg}")
>       raise err_type(err_msg)
E       RuntimeError: Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules

.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:1143: RuntimeError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
----------------------------- Captured stderr call -----------------------------
Degree max:   0%|          | 0/350 [00:00<?, ?it/s]Degree max: 100%|██████████| 350/350 [00:00<00:00, 125577.96it/s]
Degree bincount:   0%|          | 0/350 [00:00<?, ?it/s]Degree bincount: 100%|██████████| 350/350 [00:00<00:00, 101360.66it/s]
____________________ pytest_train_model_conv_head[PNAPlus] _____________________

mpnn_type = 'PNAPlus', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:97: in _
    model = get_distributed_model(
hydragnn/utils/distributed/distributed.py:379: in get_distributed_model
    model = DDP(model, **ddp_kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:802: in __init__
    self._log_and_throw(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DistributedDataParallel(
  (module): PNAPlusStack(
    (graph_convs): ModuleList(
      (0): Sequential(
        (0) -...unction): ReLU()
    (graph_shared): ModuleDict()
    (rbf): BesselBasisLayer(
      (envelope): Envelope()
    )
  )
)
err_type = <class 'RuntimeError'>
err_msg = "Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules"

    def _log_and_throw(self, err_type, err_msg):
        if self.logger is not None:
            self.logger.set_error_and_log(f"{str(err_type)}: {err_msg}")
>       raise err_type(err_msg)
E       RuntimeError: Modules with uninitialized parameters can't be used with `DistributedDataParallel`. Run a dummy forward pass to correctly initialize the modules

.venv/lib/python3.13/site-packages/torch/nn/parallel/distributed.py:1143: RuntimeError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
----------------------------- Captured stderr call -----------------------------
Degree max:   0%|          | 0/350 [00:00<?, ?it/s]Degree max: 100%|██████████| 350/350 [00:00<00:00, 125000.54it/s]
Degree bincount:   0%|          | 0/350 [00:00<?, ?it/s]Degree bincount: 100%|██████████| 350/350 [00:00<00:00, 103003.54it/s]
_____________________ pytest_train_model_conv_head[SchNet] _____________________

mpnn_type = 'SchNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mpnn_type = 'SchNet', global_attn_engine = None, global_attn_type = None
ci_input = 'ci_conv_head.json', use_lengths = False, overwrite_data = False
use_deepspeed = False, overwrite_config = None

    def unittest_train_model(
        mpnn_type,
        global_attn_engine,
        global_attn_type,
        ci_input,
        use_lengths,
        overwrite_data=False,
        use_deepspeed=False,
        overwrite_config=None,
    ):
        world_size, rank = hydragnn.utils.distributed.get_comm_size_and_rank()
    
        os.environ["SERIALIZED_DATA_PATH"] = os.getcwd()
    
        # Read in config settings and override model type.
        config_file = os.path.join(os.getcwd(), "tests/inputs", ci_input)
        with open(config_file, "r") as f:
            config = json.load(f)
        config["NeuralNetwork"]["Architecture"]["global_attn_engine"] = global_attn_engine
        config["NeuralNetwork"]["Architecture"]["global_attn_type"] = global_attn_type
        config["NeuralNetwork"]["Architecture"]["mpnn_type"] = mpnn_type
    
        # Overwrite config settings if provided
        if overwrite_config:
            config = merge_config(config, overwrite_config)
    
        """
        to test this locally, set ci.json as
        "Dataset": {
           ...
           "path": {
                   "train": "serialized_dataset/unit_test_singlehead_train.pkl",
                   "test": "serialized_dataset/unit_test_singlehead_test.pkl",
                   "validate": "serialized_dataset/unit_test_singlehead_validate.pkl"}
           ...
        """
        # use pkl files if exist by default
        for dataset_name in config["Dataset"]["path"].keys():
            if dataset_name == "total":
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + ".pkl"
                )
            else:
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + "_"
                    + dataset_name
                    + ".pkl"
                )
            if os.path.exists(pkl_file):
                config["Dataset"]["path"][dataset_name] = pkl_file
    
        # In the unit test runs, it is found MFC favors graph-level features over node-level features, compared with other models;
        # hence here we decrease the loss weight coefficient for graph-level head in MFC.
        if mpnn_type == "MFC" and ci_input == "ci_multihead.json":
            config["NeuralNetwork"]["Architecture"]["task_weights"][0] = 2
    
        # Only run with edge lengths for models that support them.
        if use_lengths:
            config["NeuralNetwork"]["Architecture"]["edge_features"] = ["lengths"]
    
        if rank == 0:
            num_samples_tot = 500
            # check if serialized pickle files or folders for raw files provided
            pkl_input = False
            if list(config["Dataset"]["path"].values())[0].endswith(".pkl"):
                pkl_input = True
            # only generate new datasets, if not pkl
            if not pkl_input:
                for dataset_name, data_path in config["Dataset"]["path"].items():
                    if overwrite_data:
                        shutil.rmtree(data_path)
                    if not os.path.exists(data_path):
                        os.makedirs(data_path)
                    if dataset_name == "total":
                        num_samples = num_samples_tot
                    elif dataset_name == "train":
                        num_samples = int(
                            num_samples_tot
                            * config["NeuralNetwork"]["Training"]["perc_train"]
                        )
                    elif dataset_name == "test":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    elif dataset_name == "validate":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    if not os.listdir(data_path):
                        tests.deterministic_graph_data(
                            data_path, number_configurations=num_samples
                        )
        MPI.COMM_WORLD.Barrier()
    
        # Since the config file uses PNA already, test the file overload here.
        # All the other models need to use the locally modified dictionary.
        if mpnn_type == "PNA" and not use_lengths:
            hydragnn.run_training(config_file, use_deepspeed)
        else:
            hydragnn.run_training(config, use_deepspeed)
    
        (
            error,
            error_mse_task,
            true_values,
            predicted_values,
        ) = hydragnn.run_prediction(config, use_deepspeed)
    
        # Set RMSE and sample MAE error thresholds
        thresholds = {
            "SAGE": [0.20, 0.20],
            "PNA": [0.20, 0.20],
            "PNAPlus": [0.20, 0.20],
            "MFC": [0.20, 0.30],
            "GIN": [0.25, 0.20],
            "GAT": [0.60, 0.70],
            "CGCNN": [0.50, 0.40],
            "SchNet": [0.20, 0.20],
            "DimeNet": [0.50, 0.50],
            "EGNN": [0.20, 0.20],
            "PNAEq": [0.60, 0.60],
            "PAINN": [0.60, 0.60],
            "MACE": [0.60, 0.70],
        }
        if use_lengths and ("vector" not in ci_input):
            thresholds["CGCNN"] = [0.175, 0.175]
            thresholds["PNA"] = [0.10, 0.10]
            thresholds["PNAPlus"] = [0.10, 0.10]
        if use_lengths and "vector" in ci_input:
            thresholds["PNA"] = [0.2, 0.15]
            thresholds["PNAPlus"] = [0.2, 0.15]
        if ci_input == "ci_conv_head.json":
            thresholds["GIN"] = [0.26, 0.51]
            thresholds["SchNet"] = [0.30, 0.30]
    
        verbosity = 2
    
        for ihead in range(len(true_values)):
            error_head_mse = error_mse_task[ihead]
            error_str = (
                str("{:.6f}".format(error_head_mse)) + " < " + str(thresholds[mpnn_type][0])
            )
            hydragnn.utils.print.print_distributed(verbosity, "head: " + error_str)
>           assert (
                error_head_mse < thresholds[mpnn_type][0]
            ), "Head RMSE checking failed for " + str(ihead)
E           AssertionError: Head RMSE checking failed for 0
E           assert tensor(0.4173) < 0.3

tests/test_graphs.py:178: AssertionError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
----------------------------- Captured stderr call -----------------------------
0: Load existing model: ./logs/SchNet-r-2.0-ncl-2-hd-20-ne-100-lr-0.02-bs-32-data-unit_test-node_ft--task_weights-1.0-/SchNet-r-2.0-ncl-2-hd-20-ne-100-lr-0.02-bs-32-data-unit_test-node_ft--task_weights-1.0-.pk
0: head: 0.417273 < 0.3
____________________ pytest_train_model_conv_head[DimeNet] _____________________

mpnn_type = 'DimeNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:89: in _
    model = create_model_config(
hydragnn/models/create.py:45: in create_model_config
    return create_model(
hydragnn/models/create.py:369: in create_model
    model = DIMEStack(
hydragnn/models/DIMEStack.py:68: in __init__
    super().__init__(input_args, conv_args, *args, **kwargs)
hydragnn/models/Base.py:174: in __init__
    self._init_conv()
hydragnn/models/DIMEStack.py:80: in _init_conv
    self.get_conv(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = DIMEStack(
  (graph_convs): ModuleList()
  (feature_layers): ModuleList()
  (heads_NN): ModuleList()
  (convs_node_hid...eDict()
  (convs_node_output): ModuleDict()
  (batch_norms_node_output): ModuleDict()
  (activation_function): ReLU()
)
input_dim = 0, output_dim = 20, edge_dim = None

    def get_conv(self, input_dim, output_dim, edge_dim=None):
        hidden_dim = output_dim if input_dim == 1 else input_dim
        assert (
>           hidden_dim > 1
        ), "DimeNet requires more than one hidden dimension between input_dim and output_dim."
E       AssertionError: DimeNet requires more than one hidden dimension between input_dim and output_dim.

hydragnn/models/DIMEStack.py:99: AssertionError
______________________ pytest_train_model_conv_head[EGNN] ______________________

mpnn_type = 'EGNN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

mpnn_type = 'EGNN', global_attn_engine = None, global_attn_type = None
ci_input = 'ci_conv_head.json', use_lengths = False, overwrite_data = False
use_deepspeed = False, overwrite_config = None

    def unittest_train_model(
        mpnn_type,
        global_attn_engine,
        global_attn_type,
        ci_input,
        use_lengths,
        overwrite_data=False,
        use_deepspeed=False,
        overwrite_config=None,
    ):
        world_size, rank = hydragnn.utils.distributed.get_comm_size_and_rank()
    
        os.environ["SERIALIZED_DATA_PATH"] = os.getcwd()
    
        # Read in config settings and override model type.
        config_file = os.path.join(os.getcwd(), "tests/inputs", ci_input)
        with open(config_file, "r") as f:
            config = json.load(f)
        config["NeuralNetwork"]["Architecture"]["global_attn_engine"] = global_attn_engine
        config["NeuralNetwork"]["Architecture"]["global_attn_type"] = global_attn_type
        config["NeuralNetwork"]["Architecture"]["mpnn_type"] = mpnn_type
    
        # Overwrite config settings if provided
        if overwrite_config:
            config = merge_config(config, overwrite_config)
    
        """
        to test this locally, set ci.json as
        "Dataset": {
           ...
           "path": {
                   "train": "serialized_dataset/unit_test_singlehead_train.pkl",
                   "test": "serialized_dataset/unit_test_singlehead_test.pkl",
                   "validate": "serialized_dataset/unit_test_singlehead_validate.pkl"}
           ...
        """
        # use pkl files if exist by default
        for dataset_name in config["Dataset"]["path"].keys():
            if dataset_name == "total":
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + ".pkl"
                )
            else:
                pkl_file = (
                    os.environ["SERIALIZED_DATA_PATH"]
                    + "/serialized_dataset/"
                    + config["Dataset"]["name"]
                    + "_"
                    + dataset_name
                    + ".pkl"
                )
            if os.path.exists(pkl_file):
                config["Dataset"]["path"][dataset_name] = pkl_file
    
        # In the unit test runs, it is found MFC favors graph-level features over node-level features, compared with other models;
        # hence here we decrease the loss weight coefficient for graph-level head in MFC.
        if mpnn_type == "MFC" and ci_input == "ci_multihead.json":
            config["NeuralNetwork"]["Architecture"]["task_weights"][0] = 2
    
        # Only run with edge lengths for models that support them.
        if use_lengths:
            config["NeuralNetwork"]["Architecture"]["edge_features"] = ["lengths"]
    
        if rank == 0:
            num_samples_tot = 500
            # check if serialized pickle files or folders for raw files provided
            pkl_input = False
            if list(config["Dataset"]["path"].values())[0].endswith(".pkl"):
                pkl_input = True
            # only generate new datasets, if not pkl
            if not pkl_input:
                for dataset_name, data_path in config["Dataset"]["path"].items():
                    if overwrite_data:
                        shutil.rmtree(data_path)
                    if not os.path.exists(data_path):
                        os.makedirs(data_path)
                    if dataset_name == "total":
                        num_samples = num_samples_tot
                    elif dataset_name == "train":
                        num_samples = int(
                            num_samples_tot
                            * config["NeuralNetwork"]["Training"]["perc_train"]
                        )
                    elif dataset_name == "test":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    elif dataset_name == "validate":
                        num_samples = int(
                            num_samples_tot
                            * (1 - config["NeuralNetwork"]["Training"]["perc_train"])
                            * 0.5
                        )
                    if not os.listdir(data_path):
                        tests.deterministic_graph_data(
                            data_path, number_configurations=num_samples
                        )
        MPI.COMM_WORLD.Barrier()
    
        # Since the config file uses PNA already, test the file overload here.
        # All the other models need to use the locally modified dictionary.
        if mpnn_type == "PNA" and not use_lengths:
            hydragnn.run_training(config_file, use_deepspeed)
        else:
            hydragnn.run_training(config, use_deepspeed)
    
        (
            error,
            error_mse_task,
            true_values,
            predicted_values,
        ) = hydragnn.run_prediction(config, use_deepspeed)
    
        # Set RMSE and sample MAE error thresholds
        thresholds = {
            "SAGE": [0.20, 0.20],
            "PNA": [0.20, 0.20],
            "PNAPlus": [0.20, 0.20],
            "MFC": [0.20, 0.30],
            "GIN": [0.25, 0.20],
            "GAT": [0.60, 0.70],
            "CGCNN": [0.50, 0.40],
            "SchNet": [0.20, 0.20],
            "DimeNet": [0.50, 0.50],
            "EGNN": [0.20, 0.20],
            "PNAEq": [0.60, 0.60],
            "PAINN": [0.60, 0.60],
            "MACE": [0.60, 0.70],
        }
        if use_lengths and ("vector" not in ci_input):
            thresholds["CGCNN"] = [0.175, 0.175]
            thresholds["PNA"] = [0.10, 0.10]
            thresholds["PNAPlus"] = [0.10, 0.10]
        if use_lengths and "vector" in ci_input:
            thresholds["PNA"] = [0.2, 0.15]
            thresholds["PNAPlus"] = [0.2, 0.15]
        if ci_input == "ci_conv_head.json":
            thresholds["GIN"] = [0.26, 0.51]
            thresholds["SchNet"] = [0.30, 0.30]
    
        verbosity = 2
    
        for ihead in range(len(true_values)):
            error_head_mse = error_mse_task[ihead]
            error_str = (
                str("{:.6f}".format(error_head_mse)) + " < " + str(thresholds[mpnn_type][0])
            )
            hydragnn.utils.print.print_distributed(verbosity, "head: " + error_str)
>           assert (
                error_head_mse < thresholds[mpnn_type][0]
            ), "Head RMSE checking failed for " + str(ihead)
E           AssertionError: Head RMSE checking failed for 0
E           assert tensor(0.2124) < 0.2

tests/test_graphs.py:178: AssertionError
----------------------------- Captured stdout call -----------------------------
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
get_autocast_and_scaler use_bf16:  False
dist.is_initialized(),sync_batch_norm,device_name: True False cpu
Using FSDP: False Sharding: ShardingStrategy.FULL_SHARD
get_autocast_and_scaler use_bf16:  False
----------------------------- Captured stderr call -----------------------------
0: Load existing model: ./logs/EGNN-r-2.0-ncl-2-hd-20-ne-100-lr-0.02-bs-32-data-unit_test-node_ft--task_weights-1.0-/EGNN-r-2.0-ncl-2-hd-20-ne-100-lr-0.02-bs-32-data-unit_test-node_ft--task_weights-1.0-.pk
0: head: 0.212449 < 0.2
_____________________ pytest_train_model_conv_head[PNAEq] ______________________

mpnn_type = 'PNAEq', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:89: in _
    model = create_model_config(
hydragnn/models/create.py:45: in create_model_config
    return create_model(
hydragnn/models/create.py:455: in create_model
    model = PNAEqStack(
hydragnn/models/PNAEqStack.py:72: in __init__
    super().__init__(input_args, conv_args, *args, **kwargs)
hydragnn/models/Base.py:174: in __init__
    self._init_conv()
hydragnn/models/PNAEqStack.py:80: in _init_conv
    self.get_conv(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = PNAEqStack(
  (graph_convs): ModuleList()
  (feature_layers): ModuleList()
  (heads_NN): ModuleList()
  (convs_node_hi...eDict()
  (convs_node_output): ModuleDict()
  (batch_norms_node_output): ModuleDict()
  (activation_function): ReLU()
)
input_dim = 0, output_dim = 20, last_layer = False, edge_dim = None

    def get_conv(self, input_dim, output_dim, last_layer=False, edge_dim=None):
        hidden_dim = output_dim if input_dim == 1 else input_dim
        assert (
>           hidden_dim > 1
        ), "PNAEq requires more than one hidden dimension between input_dim and output_dim."
E       AssertionError: PNAEq requires more than one hidden dimension between input_dim and output_dim.

hydragnn/models/PNAEqStack.py:106: AssertionError
----------------------------- Captured stderr call -----------------------------
Degree max:   0%|          | 0/350 [00:00<?, ?it/s]Degree max: 100%|██████████| 350/350 [00:00<00:00, 126617.77it/s]
Degree bincount:   0%|          | 0/350 [00:00<?, ?it/s]Degree bincount: 100%|██████████| 350/350 [00:00<00:00, 104722.96it/s]
_____________________ pytest_train_model_conv_head[PAINN] ______________________

mpnn_type = 'PAINN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:89: in _
    model = create_model_config(
hydragnn/models/create.py:45: in create_model_config
    return create_model(
hydragnn/models/create.py:428: in create_model
    model = PAINNStack(
hydragnn/models/PAINNStack.py:47: in __init__
    super().__init__(input_args, conv_args, *args, **kwargs)
hydragnn/models/Base.py:174: in __init__
    self._init_conv()
hydragnn/models/PAINNStack.py:53: in _init_conv
    self.get_conv(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = PAINNStack(
  (graph_convs): ModuleList()
  (feature_layers): ModuleList()
  (heads_NN): ModuleList()
  (convs_node_hi...eDict()
  (convs_node_output): ModuleDict()
  (batch_norms_node_output): ModuleDict()
  (activation_function): ReLU()
)
input_dim = 0, output_dim = 20, last_layer = False, edge_dim = None

    def get_conv(self, input_dim, output_dim, last_layer=False, edge_dim=None):
        hidden_dim = output_dim if input_dim == 1 else input_dim
        assert (
>           hidden_dim > 1
        ), "PainnNet requires more than one hidden dimension between input_dim and output_dim."
E       AssertionError: PainnNet requires more than one hidden dimension between input_dim and output_dim.

hydragnn/models/PAINNStack.py:79: AssertionError
=========================== short test summary info ============================
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[PNA] - Assertion...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[SAGE] - RuntimeErro...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[GAT] - RuntimeError...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[MFC] - RuntimeError...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNA] - RuntimeError...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNAPlus] - RuntimeE...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[SchNet] - Assertion...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[DimeNet] - Assertio...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[EGNN] - AssertionEr...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNAEq] - AssertionE...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PAINN] - AssertionE...
================== 11 failed, 57 passed in 564.16s (0:09:24) ===================
