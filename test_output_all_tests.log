============================= test session starts ==============================
platform darwin -- Python 3.13.7, pytest-8.4.2, pluggy-1.6.0 -- /Users/7ml/Documents/Codes/HydraGNN/.venv/bin/python
cachedir: .pytest_cache
rootdir: /Users/7ml/Documents/Codes/HydraGNN
configfile: pytest.ini
collecting ... collected 172 items

tests/test_atomicdescriptors.py::pytest_atomicdescriptors FAILED         [  0%]
tests/test_config.py::pytest_config[lsms/lsms.json] PASSED               [  1%]
tests/test_datasetclass_inheritance.py::pytest_train_model_vectoroutput[PNA] SKIPPED [  1%]
tests/test_deepspeed.py::pytest_train_model_vectoroutput_w_deepspeed[PNA] SKIPPED [  2%]
tests/test_deepspeed.py::pytest_train_model_vectoroutput_w_deepspeed_global_attention[PNA-multihead-GPS] SKIPPED [  2%]
tests/test_enthalpy.py::pytest_formation_enthalpy PASSED                 [  3%]
tests/test_examples.py::pytest_examples_energy[qm9-SAGE-multihead-GPS] FAILED [  4%]
tests/test_examples.py::pytest_examples_energy[qm9-GIN-multihead-GPS] FAILED [  4%]
tests/test_examples.py::pytest_examples_energy[qm9-GAT-multihead-GPS] FAILED [  5%]
tests/test_examples.py::pytest_examples_energy[qm9-MFC-multihead-GPS] FAILED [  5%]
tests/test_examples.py::pytest_examples_energy[qm9-PNA-multihead-GPS] FAILED [  6%]
tests/test_examples.py::pytest_examples_energy[qm9-PNAPlus-multihead-GPS] FAILED [  6%]
tests/test_examples.py::pytest_examples_energy[qm9-SchNet-multihead-GPS] FAILED [  7%]
tests/test_examples.py::pytest_examples_energy[qm9-DimeNet-multihead-GPS] FAILED [  8%]
tests/test_examples.py::pytest_examples_energy[qm9-EGNN-multihead-GPS] FAILED [  8%]
tests/test_examples.py::pytest_examples_energy[qm9-PNAEq-multihead-GPS] FAILED [  9%]
tests/test_examples.py::pytest_examples_energy[qm9-PAINN-multihead-GPS] FAILED [  9%]
tests/test_examples.py::pytest_examples_energy[md17-SAGE-multihead-GPS] FAILED [ 10%]
tests/test_examples.py::pytest_examples_energy[md17-GIN-multihead-GPS] FAILED [ 11%]
tests/test_examples.py::pytest_examples_energy[md17-GAT-multihead-GPS] FAILED [ 11%]
tests/test_examples.py::pytest_examples_energy[md17-MFC-multihead-GPS] FAILED [ 12%]
tests/test_examples.py::pytest_examples_energy[md17-PNA-multihead-GPS] FAILED [ 12%]
tests/test_examples.py::pytest_examples_energy[md17-PNAPlus-multihead-GPS] FAILED [ 13%]
tests/test_examples.py::pytest_examples_energy[md17-SchNet-multihead-GPS] FAILED [ 13%]
tests/test_examples.py::pytest_examples_energy[md17-DimeNet-multihead-GPS] FAILED [ 14%]
tests/test_examples.py::pytest_examples_energy[md17-EGNN-multihead-GPS] FAILED [ 15%]
tests/test_examples.py::pytest_examples_energy[md17-PNAEq-multihead-GPS] FAILED [ 15%]
tests/test_examples.py::pytest_examples_energy[md17-PAINN-multihead-GPS] FAILED [ 16%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PNAPlus] FAILED [ 16%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-SchNet] FAILED [ 17%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-DimeNet] FAILED [ 18%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-EGNN] FAILED [ 18%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PNAEq] FAILED [ 19%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PAINN] FAILED [ 19%]
tests/test_examples.py::pytest_examples_grad_forces[LennardJones-MACE] FAILED [ 20%]
tests/test_feature_config.py::test_parse_new_format PASSED               [ 20%]
tests/test_feature_config.py::test_parse_legacy_format PASSED            [ 21%]
tests/test_feature_config.py::test_validate_config_valid PASSED          [ 22%]
tests/test_feature_config.py::test_validate_config_mismatched_lengths PASSED [ 22%]
tests/test_feature_config.py::test_validate_config_invalid_indices PASSED [ 23%]
tests/test_feature_config.py::test_validate_against_data PASSED          [ 23%]
tests/test_feature_config.py::test_validate_against_data_mismatch PASSED [ 24%]
tests/test_feature_config.py::test_update_var_config PASSED              [ 25%]
tests/test_feature_config.py::test_get_feature_schema_example PASSED     [ 25%]
tests/test_feature_config.py::test_print_feature_summary PASSED          [ 26%]
tests/test_feature_config.py::test_empty_config PASSED                   [ 26%]
tests/test_feature_config.py::test_missing_config_raises PASSED          [ 27%]
tests/test_feature_config_validation.py::test_validate_node_feature_columns_simple PASSED [ 27%]
tests/test_feature_config_validation.py::test_validate_node_feature_columns_with_column_indices PASSED [ 28%]
tests/test_feature_config_validation.py::test_validate_node_feature_columns_edge_cases PASSED [ 29%]
tests/test_forces_equivariant.py::pytest_examples[SchNet-LennardJones] FAILED [ 29%]
tests/test_forces_equivariant.py::pytest_examples[EGNN-LennardJones] FAILED [ 30%]
tests/test_forces_equivariant.py::pytest_examples[DimeNet-LennardJones] FAILED [ 30%]
tests/test_forces_equivariant.py::pytest_examples[PAINN-LennardJones] FAILED [ 31%]
tests/test_forces_equivariant.py::pytest_examples[PNAPlus-LennardJones] FAILED [ 31%]
tests/test_forces_equivariant.py::pytest_examples[MACE-LennardJones] FAILED [ 32%]
tests/test_graphs.py::pytest_train_model[ci.json-SAGE] FAILED            [ 33%]
tests/test_graphs.py::pytest_train_model[ci.json-GIN] FAILED             [ 33%]
tests/test_graphs.py::pytest_train_model[ci.json-GAT] FAILED             [ 34%]
tests/test_graphs.py::pytest_train_model[ci.json-MFC] FAILED             [ 34%]
tests/test_graphs.py::pytest_train_model[ci.json-PNA] FAILED             [ 35%]
tests/test_graphs.py::pytest_train_model[ci.json-PNAPlus] FAILED         [ 36%]
tests/test_graphs.py::pytest_train_model[ci.json-CGCNN] FAILED           [ 36%]
tests/test_graphs.py::pytest_train_model[ci.json-SchNet] FAILED          [ 37%]
tests/test_graphs.py::pytest_train_model[ci.json-DimeNet] FAILED         [ 37%]
tests/test_graphs.py::pytest_train_model[ci.json-EGNN] FAILED            [ 38%]
tests/test_graphs.py::pytest_train_model[ci.json-PNAEq] FAILED           [ 38%]
tests/test_graphs.py::pytest_train_model[ci.json-PAINN] FAILED           [ 39%]
tests/test_graphs.py::pytest_train_model[ci.json-MACE] FAILED            [ 40%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-SAGE] FAILED  [ 40%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-GIN] FAILED   [ 41%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-GAT] FAILED   [ 41%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-MFC] FAILED   [ 42%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNA] FAILED   [ 43%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAPlus] FAILED [ 43%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-CGCNN] FAILED [ 44%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-SchNet] FAILED [ 44%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-DimeNet] FAILED [ 45%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-EGNN] FAILED  [ 45%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAEq] FAILED [ 46%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-PAINN] FAILED [ 47%]
tests/test_graphs.py::pytest_train_model[ci_multihead.json-MACE] FAILED  [ 47%]
tests/test_graphs.py::pytest_train_model_lengths[GAT] FAILED             [ 48%]
tests/test_graphs.py::pytest_train_model_lengths[PNA] FAILED             [ 48%]
tests/test_graphs.py::pytest_train_model_lengths[PNAPlus] FAILED         [ 49%]
tests/test_graphs.py::pytest_train_model_lengths[CGCNN] FAILED           [ 50%]
tests/test_graphs.py::pytest_train_model_lengths[SchNet] FAILED          [ 50%]
tests/test_graphs.py::pytest_train_model_lengths[DimeNet] FAILED         [ 51%]
tests/test_graphs.py::pytest_train_model_lengths[EGNN] FAILED            [ 51%]
tests/test_graphs.py::pytest_train_model_lengths[PNAEq] FAILED           [ 52%]
tests/test_graphs.py::pytest_train_model_lengths[PAINN] FAILED           [ 52%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[GAT-multihead-GPS] FAILED [ 53%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNA-multihead-GPS] FAILED [ 54%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAPlus-multihead-GPS] FAILED [ 54%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[CGCNN-multihead-GPS] FAILED [ 55%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[SchNet-multihead-GPS] FAILED [ 55%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[DimeNet-multihead-GPS] FAILED [ 56%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[EGNN-multihead-GPS] FAILED [ 56%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAEq-multihead-GPS] FAILED [ 57%]
tests/test_graphs.py::pytest_train_model_lengths_global_attention[PAINN-multihead-GPS] FAILED [ 58%]
tests/test_graphs.py::pytest_train_mace_model_lengths[MACE] FAILED       [ 58%]
tests/test_graphs.py::pytest_train_equivariant_model[EGNN] FAILED        [ 59%]
tests/test_graphs.py::pytest_train_equivariant_model[SchNet] FAILED      [ 59%]
tests/test_graphs.py::pytest_train_equivariant_model[PNAEq] FAILED       [ 60%]
tests/test_graphs.py::pytest_train_equivariant_model[PAINN] FAILED       [ 61%]
tests/test_graphs.py::pytest_train_equivariant_model[MACE] FAILED        [ 61%]
tests/test_graphs.py::pytest_train_model_vectoroutput[GAT] FAILED        [ 62%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNA] FAILED        [ 62%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNAPlus] FAILED    [ 63%]
tests/test_graphs.py::pytest_train_model_vectoroutput[SchNet] FAILED     [ 63%]
tests/test_graphs.py::pytest_train_model_vectoroutput[DimeNet] FAILED    [ 64%]
tests/test_graphs.py::pytest_train_model_vectoroutput[EGNN] FAILED       [ 65%]
tests/test_graphs.py::pytest_train_model_vectoroutput[PNAEq] FAILED      [ 65%]
tests/test_graphs.py::pytest_train_model_conv_head[SAGE] FAILED          [ 66%]
tests/test_graphs.py::pytest_train_model_conv_head[GIN] FAILED           [ 66%]
tests/test_graphs.py::pytest_train_model_conv_head[GAT] FAILED           [ 67%]
tests/test_graphs.py::pytest_train_model_conv_head[MFC] FAILED           [ 68%]
tests/test_graphs.py::pytest_train_model_conv_head[PNA] FAILED           [ 68%]
tests/test_graphs.py::pytest_train_model_conv_head[PNAPlus] FAILED       [ 69%]
tests/test_graphs.py::pytest_train_model_conv_head[SchNet] FAILED        [ 69%]
tests/test_graphs.py::pytest_train_model_conv_head[DimeNet] FAILED       [ 70%]
tests/test_graphs.py::pytest_train_model_conv_head[EGNN] FAILED          [ 70%]
tests/test_graphs.py::pytest_train_model_conv_head[PNAEq] FAILED         [ 71%]
tests/test_graphs.py::pytest_train_model_conv_head[PAINN] FAILED         [ 72%]
tests/test_interatomic_potential.py::pytest_model_creation_with_enhancement PASSED [ 72%]
tests/test_interatomic_potential.py::pytest_forward_pass PASSED          [ 73%]
tests/test_interatomic_potential.py::pytest_energy_force_consistency PASSED [ 73%]
tests/test_loss_and_activation_functions.py::pytest_loss_functions[mse] FAILED [ 74%]
tests/test_loss_and_activation_functions.py::pytest_loss_functions[mae] FAILED [ 75%]
tests/test_loss_and_activation_functions.py::pytest_loss_functions[rmse] FAILED [ 75%]
tests/test_loss_and_activation_functions.py::pytest_loss_functions[GaussianNLLLoss] FAILED [ 76%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[relu] FAILED [ 76%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[selu] FAILED [ 77%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[prelu] FAILED [ 77%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[elu] FAILED [ 78%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_01] FAILED [ 79%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_025] FAILED [ 79%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_05] FAILED [ 80%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[relu] FAILED [ 80%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[selu] FAILED [ 81%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[prelu] FAILED [ 81%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[elu] FAILED [ 82%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_01] FAILED [ 83%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_025] FAILED [ 83%]
tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_05] FAILED [ 84%]
tests/test_model_loadpred.py::pytest_model_loadpred FAILED               [ 84%]
tests/test_optimizer.py::pytest_optimizers[False-SGD] FAILED             [ 85%]
tests/test_optimizer.py::pytest_optimizers[False-Adam] FAILED            [ 86%]
tests/test_optimizer.py::pytest_optimizers[False-Adadelta] FAILED        [ 86%]
tests/test_optimizer.py::pytest_optimizers[False-Adagrad] FAILED         [ 87%]
tests/test_optimizer.py::pytest_optimizers[False-Adamax] FAILED          [ 87%]
tests/test_optimizer.py::pytest_optimizers[False-AdamW] FAILED           [ 88%]
tests/test_optimizer.py::pytest_optimizers[False-RMSprop] FAILED         [ 88%]
tests/test_optimizer.py::pytest_optimizers[True-SGD] FAILED              [ 89%]
tests/test_optimizer.py::pytest_optimizers[True-Adam] FAILED             [ 90%]
tests/test_optimizer.py::pytest_optimizers[True-Adadelta] FAILED         [ 90%]
tests/test_optimizer.py::pytest_optimizers[True-Adagrad] FAILED          [ 91%]
tests/test_optimizer.py::pytest_optimizers[True-Adamax] FAILED           [ 91%]
tests/test_optimizer.py::pytest_optimizers[True-AdamW] FAILED            [ 92%]
tests/test_optimizer.py::pytest_optimizers[True-RMSprop] FAILED          [ 93%]
tests/test_periodic_boundary_conditions.py::pytest_periodic_h2 PASSED    [ 93%]
tests/test_periodic_boundary_conditions.py::pytest_periodic_bcc_large PASSED [ 94%]
tests/test_radial_transforms.py::pytest_train_model_transforms[None-bessel-MACE] FAILED [ 94%]
tests/test_radial_transforms.py::pytest_train_model_transforms[None-gaussian-MACE] FAILED [ 95%]
tests/test_radial_transforms.py::pytest_train_model_transforms[None-chebyshev-MACE] FAILED [ 95%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-bessel-MACE] FAILED [ 96%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-gaussian-MACE] FAILED [ 97%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-chebyshev-MACE] FAILED [ 97%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-bessel-MACE] FAILED [ 98%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-gaussian-MACE] FAILED [ 98%]
tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-chebyshev-MACE] FAILED [ 99%]
tests/test_rotational_invariance.py::pytest_rotational_invariance PASSED [100%]

=================================== FAILURES ===================================
___________________________ pytest_atomicdescriptors ___________________________

    @pytest.mark.mpi_skip()
    def pytest_atomicdescriptors():
        file_path = os.path.join(
            os.path.dirname(__file__),
            "..",
            "hydragnn/utils/descriptors_and_embeddings/atomicdescriptors.py",
        )
        return_code = subprocess.call(["python", file_path])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_atomicdescriptors.py:28: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../hydragnn/utils/descriptors_and_embeddings/atomicdescriptors.py", line 231, in <module>
    atomicdescriptor = atomicdescriptors(
        "./embedding.json", overwritten=True, element_types=["C", "H", "S"]
    )
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../hydragnn/utils/descriptors_and_embeddings/atomicdescriptors.py", line 32, in __init__
    for ele in mendeleev.get_all_elements():
               ^^^^^^^^^
NameError: name 'mendeleev' is not defined
________________ pytest_examples_energy[qm9-SAGE-multihead-GPS] ________________

example = 'qm9', mpnn_type = 'SAGE', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[qm9-GIN-multihead-GPS] _________________

example = 'qm9', mpnn_type = 'GIN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[qm9-GAT-multihead-GPS] _________________

example = 'qm9', mpnn_type = 'GAT', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[qm9-MFC-multihead-GPS] _________________

example = 'qm9', mpnn_type = 'MFC', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[qm9-PNA-multihead-GPS] _________________

example = 'qm9', mpnn_type = 'PNA', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_energy[qm9-PNAPlus-multihead-GPS] _______________

example = 'qm9', mpnn_type = 'PNAPlus', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[qm9-SchNet-multihead-GPS] _______________

example = 'qm9', mpnn_type = 'SchNet', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_energy[qm9-DimeNet-multihead-GPS] _______________

example = 'qm9', mpnn_type = 'DimeNet', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[qm9-EGNN-multihead-GPS] ________________

example = 'qm9', mpnn_type = 'EGNN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[qm9-PNAEq-multihead-GPS] ________________

example = 'qm9', mpnn_type = 'PNAEq', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[qm9-PAINN-multihead-GPS] ________________

example = 'qm9', mpnn_type = 'PAINN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/qm9/qm9.py", line 16, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[md17-SAGE-multihead-GPS] ________________

example = 'md17', mpnn_type = 'SAGE', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[md17-GIN-multihead-GPS] ________________

example = 'md17', mpnn_type = 'GIN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[md17-GAT-multihead-GPS] ________________

example = 'md17', mpnn_type = 'GAT', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[md17-MFC-multihead-GPS] ________________

example = 'md17', mpnn_type = 'MFC', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_energy[md17-PNA-multihead-GPS] ________________

example = 'md17', mpnn_type = 'PNA', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_energy[md17-PNAPlus-multihead-GPS] ______________

example = 'md17', mpnn_type = 'PNAPlus', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_energy[md17-SchNet-multihead-GPS] _______________

example = 'md17', mpnn_type = 'SchNet', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_energy[md17-DimeNet-multihead-GPS] ______________

example = 'md17', mpnn_type = 'DimeNet', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[md17-EGNN-multihead-GPS] ________________

example = 'md17', mpnn_type = 'EGNN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[md17-PNAEq-multihead-GPS] _______________

example = 'md17', mpnn_type = 'PNAEq', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_energy[md17-PAINN-multihead-GPS] _______________

example = 'md17', mpnn_type = 'PAINN', global_attn_engine = 'GPS'
global_attn_type = 'multihead'

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    @pytest.mark.parametrize("example", ["qm9", "md17"])
    @pytest.mark.mpi_skip()
    def pytest_examples_energy(example, mpnn_type, global_attn_engine, global_attn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(
            [
                "python",
                file_path,
                "--mpnn_type",
                mpnn_type,
                "--global_attn_engine",
                global_attn_engine,
                "--global_attn_type",
                global_attn_type,
            ]
        )
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:59: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/md17/md17.py", line 17, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_grad_forces[LennardJones-PNAPlus] _______________

example = 'LennardJones', mpnn_type = 'PNAPlus'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_grad_forces[LennardJones-SchNet] _______________

example = 'LennardJones', mpnn_type = 'SchNet'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________ pytest_examples_grad_forces[LennardJones-DimeNet] _______________

example = 'LennardJones', mpnn_type = 'DimeNet'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_grad_forces[LennardJones-EGNN] ________________

example = 'LennardJones', mpnn_type = 'EGNN'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_grad_forces[LennardJones-PNAEq] ________________

example = 'LennardJones', mpnn_type = 'PNAEq'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________ pytest_examples_grad_forces[LennardJones-PAINN] ________________

example = 'LennardJones', mpnn_type = 'PAINN'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
________________ pytest_examples_grad_forces[LennardJones-MACE] ________________

example = 'LennardJones', mpnn_type = 'MACE'

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.mpi_skip()
    def pytest_examples_grad_forces(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")
    
        # Add the --mpnn_type argument for the subprocess call
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_examples.py:87: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_____________________ pytest_examples[SchNet-LennardJones] _____________________

example = 'LennardJones', mpnn_type = 'SchNet'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________________ pytest_examples[EGNN-LennardJones] ______________________

example = 'LennardJones', mpnn_type = 'EGNN'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
____________________ pytest_examples[DimeNet-LennardJones] _____________________

example = 'LennardJones', mpnn_type = 'DimeNet'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_____________________ pytest_examples[PAINN-LennardJones] ______________________

example = 'LennardJones', mpnn_type = 'PAINN'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
____________________ pytest_examples[PNAPlus-LennardJones] _____________________

example = 'LennardJones', mpnn_type = 'PNAPlus'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
______________________ pytest_examples[MACE-LennardJones] ______________________

example = 'LennardJones', mpnn_type = 'MACE'

    @pytest.mark.parametrize("example", ["LennardJones"])
    @pytest.mark.parametrize(
        "mpnn_type", ["SchNet", "EGNN", "DimeNet", "PAINN", "PNAPlus", "MACE"]
    )
    @pytest.mark.mpi_skip()
    def pytest_examples(example, mpnn_type):
        path = os.path.join(os.path.dirname(__file__), "..", "examples", example)
        file_path = os.path.join(path, example + ".py")  # Assuming different model scripts
        return_code = subprocess.call(["python", file_path, "--mpnn_type", mpnn_type])
    
        # Check the file ran without error.
>       assert return_code == 0
E       assert 1 == 0

tests/test_forces_equivariant.py:29: AssertionError
----------------------------- Captured stderr call -----------------------------
Traceback (most recent call last):
  File "/Users/7ml/Documents/Codes/HydraGNN/tests/../examples/LennardJones/LennardJones.py", line 33, in <module>
    import hydragnn
ModuleNotFoundError: No module named 'hydragnn'
_______________________ pytest_train_model[ci.json-SAGE] _______________________

mpnn_type = 'SAGE', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-GIN] ________________________

mpnn_type = 'GIN', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-GAT] ________________________

mpnn_type = 'GAT', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-MFC] ________________________

mpnn_type = 'MFC', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-PNA] ________________________

mpnn_type = 'PNA', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:132: in unittest_train_model
    hydragnn.run_training(config_file, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:62: in _
    run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model[ci.json-PNAPlus] ______________________

mpnn_type = 'PNAPlus', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model[ci.json-CGCNN] _______________________

mpnn_type = 'CGCNN', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model[ci.json-SchNet] ______________________

mpnn_type = 'SchNet', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model[ci.json-DimeNet] ______________________

mpnn_type = 'DimeNet', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-EGNN] _______________________

mpnn_type = 'EGNN', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model[ci.json-PNAEq] _______________________

mpnn_type = 'PNAEq', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model[ci.json-PAINN] _______________________

mpnn_type = 'PAINN', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model[ci.json-MACE] _______________________

mpnn_type = 'MACE', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-SAGE] __________________

mpnn_type = 'SAGE', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-GIN] ___________________

mpnn_type = 'GIN', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-GAT] ___________________

mpnn_type = 'GAT', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-MFC] ___________________

mpnn_type = 'MFC', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-PNA] ___________________

mpnn_type = 'PNA', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:132: in unittest_train_model
    hydragnn.run_training(config_file, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:62: in _
    run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________ pytest_train_model[ci_multihead.json-PNAPlus] _________________

mpnn_type = 'PNAPlus', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_train_model[ci_multihead.json-CGCNN] __________________

mpnn_type = 'CGCNN', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_train_model[ci_multihead.json-SchNet] _________________

mpnn_type = 'SchNet', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________ pytest_train_model[ci_multihead.json-DimeNet] _________________

mpnn_type = 'DimeNet', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-EGNN] __________________

mpnn_type = 'EGNN', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_train_model[ci_multihead.json-PNAEq] __________________

mpnn_type = 'PNAEq', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_train_model[ci_multihead.json-PAINN] __________________

mpnn_type = 'PAINN', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_train_model[ci_multihead.json-MACE] __________________

mpnn_type = 'MACE', ci_input = 'ci_multihead.json', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "CGCNN",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
            "MACE",
        ],
    )
    @pytest.mark.parametrize("ci_input", ["ci.json", "ci_multihead.json"])
    def pytest_train_model(mpnn_type, ci_input, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, ci_input, False, overwrite_data)

tests/test_graphs.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model_lengths[GAT] ________________________

mpnn_type = 'GAT', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model_lengths[PNA] ________________________

mpnn_type = 'PNA', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_lengths[PNAPlus] ______________________

mpnn_type = 'PNAPlus', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_lengths[CGCNN] _______________________

mpnn_type = 'CGCNN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_lengths[SchNet] ______________________

mpnn_type = 'SchNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_lengths[DimeNet] ______________________

mpnn_type = 'DimeNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_train_model_lengths[EGNN] _______________________

mpnn_type = 'EGNN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_lengths[PNAEq] _______________________

mpnn_type = 'PNAEq', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_lengths[PAINN] _______________________

mpnn_type = 'PAINN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________ pytest_train_model_lengths_global_attention[GAT-multihead-GPS] ________

mpnn_type = 'GAT', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________ pytest_train_model_lengths_global_attention[PNA-multihead-GPS] ________

mpnn_type = 'PNA', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______ pytest_train_model_lengths_global_attention[PNAPlus-multihead-GPS] ______

mpnn_type = 'PNAPlus', global_attn_engine = 'GPS'
global_attn_type = 'multihead', overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______ pytest_train_model_lengths_global_attention[CGCNN-multihead-GPS] _______

mpnn_type = 'CGCNN', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______ pytest_train_model_lengths_global_attention[SchNet-multihead-GPS] _______

mpnn_type = 'SchNet', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______ pytest_train_model_lengths_global_attention[DimeNet-multihead-GPS] ______

mpnn_type = 'DimeNet', global_attn_engine = 'GPS'
global_attn_type = 'multihead', overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______ pytest_train_model_lengths_global_attention[EGNN-multihead-GPS] ________

mpnn_type = 'EGNN', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______ pytest_train_model_lengths_global_attention[PNAEq-multihead-GPS] _______

mpnn_type = 'PNAEq', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______ pytest_train_model_lengths_global_attention[PAINN-multihead-GPS] _______

mpnn_type = 'PAINN', global_attn_engine = 'GPS', global_attn_type = 'multihead'
overwrite_data = False

    @pytest.mark.parametrize(
        "global_attn_engine",
        ["GPS"],
    )
    @pytest.mark.parametrize("global_attn_type", ["multihead"])
    @pytest.mark.parametrize(
        "mpnn_type",
        ["GAT", "PNA", "PNAPlus", "CGCNN", "SchNet", "DimeNet", "EGNN", "PNAEq", "PAINN"],
    )
    def pytest_train_model_lengths_global_attention(
        mpnn_type, global_attn_engine, global_attn_type, overwrite_data=False
    ):
>       unittest_train_model(
            mpnn_type, global_attn_engine, global_attn_type, "ci.json", True, overwrite_data
        )

tests/test_graphs.py:248: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_mace_model_lengths[MACE] _____________________

mpnn_type = 'MACE', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        ["MACE"],
    )
    def pytest_train_mace_model_lengths(mpnn_type, overwrite_data=False):
>       unittest_train_model(mpnn_type, None, None, "ci.json", True, overwrite_data)

tests/test_graphs.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_equivariant_model[EGNN] _____________________

mpnn_type = 'EGNN', overwrite_data = False

    @pytest.mark.parametrize("mpnn_type", ["EGNN", "SchNet", "PNAEq", "PAINN", "MACE"])
    def pytest_train_equivariant_model(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_equivariant.json", False, overwrite_data
        )

tests/test_graphs.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_equivariant_model[SchNet] ____________________

mpnn_type = 'SchNet', overwrite_data = False

    @pytest.mark.parametrize("mpnn_type", ["EGNN", "SchNet", "PNAEq", "PAINN", "MACE"])
    def pytest_train_equivariant_model(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_equivariant.json", False, overwrite_data
        )

tests/test_graphs.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_equivariant_model[PNAEq] _____________________

mpnn_type = 'PNAEq', overwrite_data = False

    @pytest.mark.parametrize("mpnn_type", ["EGNN", "SchNet", "PNAEq", "PAINN", "MACE"])
    def pytest_train_equivariant_model(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_equivariant.json", False, overwrite_data
        )

tests/test_graphs.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_equivariant_model[PAINN] _____________________

mpnn_type = 'PAINN', overwrite_data = False

    @pytest.mark.parametrize("mpnn_type", ["EGNN", "SchNet", "PNAEq", "PAINN", "MACE"])
    def pytest_train_equivariant_model(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_equivariant.json", False, overwrite_data
        )

tests/test_graphs.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_equivariant_model[MACE] _____________________

mpnn_type = 'MACE', overwrite_data = False

    @pytest.mark.parametrize("mpnn_type", ["EGNN", "SchNet", "PNAEq", "PAINN", "MACE"])
    def pytest_train_equivariant_model(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_equivariant.json", False, overwrite_data
        )

tests/test_graphs.py:265: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_vectoroutput[GAT] _____________________

mpnn_type = 'GAT', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_vectoroutput[PNA] _____________________

mpnn_type = 'PNA', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
___________________ pytest_train_model_vectoroutput[PNAPlus] ___________________

mpnn_type = 'PNAPlus', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
___________________ pytest_train_model_vectoroutput[SchNet] ____________________

mpnn_type = 'SchNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
___________________ pytest_train_model_vectoroutput[DimeNet] ___________________

mpnn_type = 'DimeNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_model_vectoroutput[EGNN] _____________________

mpnn_type = 'EGNN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_model_vectoroutput[PNAEq] ____________________

mpnn_type = 'PNAEq', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "GAT",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
        ],
    )
    def pytest_train_model_vectoroutput(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_vectoroutput.json", True, overwrite_data
        )

tests/test_graphs.py:284: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[SAGE] ______________________

mpnn_type = 'SAGE', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[GIN] _______________________

mpnn_type = 'GIN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[GAT] _______________________

mpnn_type = 'GAT', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[MFC] _______________________

mpnn_type = 'MFC', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[PNA] _______________________

mpnn_type = 'PNA', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:132: in unittest_train_model
    hydragnn.run_training(config_file, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:62: in _
    run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_model_conv_head[PNAPlus] _____________________

mpnn_type = 'PNAPlus', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_conv_head[SchNet] _____________________

mpnn_type = 'SchNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_train_model_conv_head[DimeNet] _____________________

mpnn_type = 'DimeNet', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_train_model_conv_head[EGNN] ______________________

mpnn_type = 'EGNN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_conv_head[PNAEq] ______________________

mpnn_type = 'PNAEq', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________________ pytest_train_model_conv_head[PAINN] ______________________

mpnn_type = 'PAINN', overwrite_data = False

    @pytest.mark.parametrize(
        "mpnn_type",
        [
            "SAGE",
            "GIN",
            "GAT",
            "MFC",
            "PNA",
            "PNAPlus",
            "SchNet",
            "DimeNet",
            "EGNN",
            "PNAEq",
            "PAINN",
        ],
    )
    def pytest_train_model_conv_head(mpnn_type, overwrite_data=False):
>       unittest_train_model(
            mpnn_type, None, None, "ci_conv_head.json", False, overwrite_data
        )

tests/test_graphs.py:306: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_graphs.py:134: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________________ pytest_loss_functions[mse] __________________________

loss_function_type = 'mse', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "loss_function_type", ["mse", "mae", "rmse", "GaussianNLLLoss"]
    )
    def pytest_loss_functions(loss_function_type, ci_input="ci.json", overwrite_data=False):
        if loss_function_type == "GaussianNLLLoss":
            ci_input = "ci_multihead.json"
>       unittest_loss_and_activation_functions(
            "relu", loss_function_type, ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________________ pytest_loss_functions[mae] __________________________

loss_function_type = 'mae', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "loss_function_type", ["mse", "mae", "rmse", "GaussianNLLLoss"]
    )
    def pytest_loss_functions(loss_function_type, ci_input="ci.json", overwrite_data=False):
        if loss_function_type == "GaussianNLLLoss":
            ci_input = "ci_multihead.json"
>       unittest_loss_and_activation_functions(
            "relu", loss_function_type, ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________________ pytest_loss_functions[rmse] __________________________

loss_function_type = 'rmse', ci_input = 'ci.json', overwrite_data = False

    @pytest.mark.parametrize(
        "loss_function_type", ["mse", "mae", "rmse", "GaussianNLLLoss"]
    )
    def pytest_loss_functions(loss_function_type, ci_input="ci.json", overwrite_data=False):
        if loss_function_type == "GaussianNLLLoss":
            ci_input = "ci_multihead.json"
>       unittest_loss_and_activation_functions(
            "relu", loss_function_type, ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________ pytest_loss_functions[GaussianNLLLoss] ____________________

loss_function_type = 'GaussianNLLLoss', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "loss_function_type", ["mse", "mae", "rmse", "GaussianNLLLoss"]
    )
    def pytest_loss_functions(loss_function_type, ci_input="ci.json", overwrite_data=False):
        if loss_function_type == "GaussianNLLLoss":
            ci_input = "ci_multihead.json"
>       unittest_loss_and_activation_functions(
            "relu", loss_function_type, ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_activation_functions_multihead[relu] __________________

activation_function_type = 'relu', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_activation_functions_multihead[selu] __________________

activation_function_type = 'selu', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________ pytest_activation_functions_multihead[prelu] _________________

activation_function_type = 'prelu', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
__________________ pytest_activation_functions_multihead[elu] __________________

activation_function_type = 'elu', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_activation_functions_multihead[lrelu_01] ________________

activation_function_type = 'lrelu_01', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_activation_functions_multihead[lrelu_025] _______________

activation_function_type = 'lrelu_025', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_activation_functions_multihead[lrelu_05] ________________

activation_function_type = 'lrelu_05', ci_input = 'ci_multihead.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_multihead(
        activation_function_type, ci_input="ci_multihead.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________ pytest_activation_functions_vectoroutput[relu] ________________

activation_function_type = 'relu', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________ pytest_activation_functions_vectoroutput[selu] ________________

activation_function_type = 'selu', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_activation_functions_vectoroutput[prelu] ________________

activation_function_type = 'prelu', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________ pytest_activation_functions_vectoroutput[elu] _________________

activation_function_type = 'elu', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_activation_functions_vectoroutput[lrelu_01] ______________

activation_function_type = 'lrelu_01', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________ pytest_activation_functions_vectoroutput[lrelu_025] ______________

activation_function_type = 'lrelu_025', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_activation_functions_vectoroutput[lrelu_05] ______________

activation_function_type = 'lrelu_05', ci_input = 'ci_vectoroutput.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "activation_function_type",
        ["relu", "selu", "prelu", "elu", "lrelu_01", "lrelu_025", "lrelu_05"],
    )
    def pytest_activation_functions_vectoroutput(
        activation_function_type, ci_input="ci_vectoroutput.json", overwrite_data=False
    ):
>       unittest_loss_and_activation_functions(
            activation_function_type, "mse", ci_input, overwrite_data
        )

tests/test_loss_and_activation_functions.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_loss_and_activation_functions.py:100: in unittest_loss_and_activation_functions
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
____________________________ pytest_model_loadpred _____________________________

    def pytest_model_loadpred():
        model_type = "PNA"
        ci_input = "ci_multihead.json"
        config_file = os.path.join(os.getcwd(), "tests/inputs", ci_input)
        with open(config_file, "r") as f:
            config = json.load(f)
        config["NeuralNetwork"]["Architecture"]["model_type"] = model_type
        # Update var_config with features before accessing input_node_features
        var_config = config["NeuralNetwork"]["Variables_of_interest"]
        var_config = update_var_config_with_features(var_config)
        config["NeuralNetwork"]["Variables_of_interest"] = var_config
        # get the directory of trained model
        log_name = hydragnn.utils.input_config_parsing.config_utils.get_log_name_config(
            config
        )
        modelfile = os.path.join("./logs/", log_name, log_name + ".pk")
        # check if pretrained model and pkl datasets files exists
        case_exist = True
        config_file = os.path.join("./logs/", log_name, "config.json")
        if not (os.path.isfile(modelfile) and os.path.isfile(config_file)):
            print("Model or configure file not found: ", modelfile, config_file)
            case_exist = False
        else:
            with open(config_file, "r") as f:
                config = json.load(f)
            # Update var_config after loading saved config
            var_config = config["NeuralNetwork"]["Variables_of_interest"]
            var_config = update_var_config_with_features(var_config)
            config["NeuralNetwork"]["Variables_of_interest"] = var_config
            for dataset_name, raw_data_path in config["Dataset"]["path"].items():
                if not os.path.isfile(raw_data_path):
                    print(dataset_name, "datasets not found: ", raw_data_path)
                    case_exist = False
                    break
        if not case_exist:
            unittest_train_model(
                config["NeuralNetwork"]["Architecture"]["model_type"],
                "ci_multihead.json",
                False,
                False,
            )
>       unittest_model_prediction(config)

tests/test_model_loadpred.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_model_loadpred.py:23: in unittest_model_prediction
    _, _ = hydragnn.utils.distributed.setup_ddp()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________________ pytest_optimizers[False-SGD] _________________________

optimizer_type = 'SGD', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________________ pytest_optimizers[False-Adam] _________________________

optimizer_type = 'Adam', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________________ pytest_optimizers[False-Adadelta] _______________________

optimizer_type = 'Adadelta', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[False-Adagrad] _______________________

optimizer_type = 'Adagrad', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[False-Adamax] ________________________

optimizer_type = 'Adamax', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________________ pytest_optimizers[False-AdamW] ________________________

optimizer_type = 'AdamW', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[False-RMSprop] _______________________

optimizer_type = 'RMSprop', use_zero_redundancy = False, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________________ pytest_optimizers[True-SGD] __________________________

optimizer_type = 'SGD', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_________________________ pytest_optimizers[True-Adam] _________________________

optimizer_type = 'Adam', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[True-Adadelta] _______________________

optimizer_type = 'Adadelta', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[True-Adagrad] ________________________

optimizer_type = 'Adagrad', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________________ pytest_optimizers[True-Adamax] ________________________

optimizer_type = 'Adamax', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
________________________ pytest_optimizers[True-AdamW] _________________________

optimizer_type = 'AdamW', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________________ pytest_optimizers[True-RMSprop] ________________________

optimizer_type = 'RMSprop', use_zero_redundancy = True, ci_input = 'ci.json'
overwrite_data = False

    @pytest.mark.parametrize(
        "optimizer_type",
        ["SGD", "Adam", "Adadelta", "Adagrad", "Adamax", "AdamW", "RMSprop"],
    )
    @pytest.mark.parametrize(
        "use_zero_redundancy",
        [False, True],
    )
    def pytest_optimizers(
        optimizer_type, use_zero_redundancy, ci_input="ci.json", overwrite_data=False
    ):
>       unittest_optimizers(optimizer_type, use_zero_redundancy, ci_input, overwrite_data)

tests/test_optimizer.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_optimizer.py:94: in unittest_optimizers
    hydragnn.run_training(config)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_train_model_transforms[None-bessel-MACE] ________________

model_type = 'MACE', basis_function = 'bessel', distance_transform = 'None'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_train_model_transforms[None-gaussian-MACE] _______________

model_type = 'MACE', basis_function = 'gaussian', distance_transform = 'None'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_train_model_transforms[None-chebyshev-MACE] ______________

model_type = 'MACE', basis_function = 'chebyshev', distance_transform = 'None'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_train_model_transforms[Agnesi-bessel-MACE] _______________

model_type = 'MACE', basis_function = 'bessel', distance_transform = 'Agnesi'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________ pytest_train_model_transforms[Agnesi-gaussian-MACE] ______________

model_type = 'MACE', basis_function = 'gaussian', distance_transform = 'Agnesi'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_____________ pytest_train_model_transforms[Agnesi-chebyshev-MACE] _____________

model_type = 'MACE', basis_function = 'chebyshev', distance_transform = 'Agnesi'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
_______________ pytest_train_model_transforms[Soft-bessel-MACE] ________________

model_type = 'MACE', basis_function = 'bessel', distance_transform = 'Soft'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_train_model_transforms[Soft-gaussian-MACE] _______________

model_type = 'MACE', basis_function = 'gaussian', distance_transform = 'Soft'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
______________ pytest_train_model_transforms[Soft-chebyshev-MACE] ______________

model_type = 'MACE', basis_function = 'chebyshev', distance_transform = 'Soft'
use_lengths = True, overwrite_data = False

    @pytest.mark.parametrize(
        "model_type",
        ["MACE"],
    )
    @pytest.mark.parametrize("basis_function", ["bessel", "gaussian", "chebyshev"])
    @pytest.mark.parametrize("distance_transform", ["None", "Agnesi", "Soft"])
    def pytest_train_model_transforms(
        model_type,
        basis_function,
        distance_transform,
        use_lengths=True,
        overwrite_data=False,
    ):
>       unittest_train_model(
            model_type,
            basis_function,
            distance_transform,
            "ci.json",
            use_lengths,
            overwrite_data,
        )

tests/test_radial_transforms.py:201: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/test_radial_transforms.py:130: in unittest_train_model
    hydragnn.run_training(config, use_deepspeed)
/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/functools.py:934: in wrapper
    return dispatch(args[0].__class__)(*args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/run_training.py:80: in _
    world_size, world_rank = setup_ddp(use_deepspeed=use_deepspeed)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
hydragnn/utils/distributed/distributed.py:206: in setup_ddp
    dist.init_process_group(
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:81: in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/c10d_logger.py:95: in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:1757: in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:278: in _env_rendezvous_handler
    store = _create_c10d_store(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

hostname = '127.0.0.1', port = 8889, rank = 0, world_size = 1
timeout = datetime.timedelta(seconds=1800), use_libuv = True

    def _create_c10d_store(
        hostname, port, rank, world_size, timeout, use_libuv=True
    ) -> Store:
        """
        Smartly creates a c10d Store object on ``rank`` based on whether we need to reuse agent store.
    
        The TCPStore server is assumed to be hosted
        on ``hostname:port``.
    
        By default, the TCPStore server uses the asynchronous implementation
        ``LibUVStoreDaemon`` which utilizes libuv.
    
        If ``torchelastic_use_agent_store()`` is ``True``, then it is assumed that
        the agent leader (node rank 0) hosts the TCPStore server (for which the
        endpoint is specified by the given ``hostname:port``). Hence
        ALL ranks will create and return a TCPStore client (e.g. ``start_daemon=False``).
    
        If ``torchelastic_use_agent_store()`` is ``False``, then rank 0 will host
        the TCPStore (with multi-tenancy) and it is assumed that rank 0's hostname
        and port are correctly passed via ``hostname`` and ``port``. All
        non-zero ranks will create and return a TCPStore client.
        """
        # check if port is uint16_t
        if not 0 <= port < 2**16:
            raise ValueError(f"port must have value from 0 to 65535 but was {port}.")
    
        if _torchelastic_use_agent_store():
            # We create a new TCPStore for every retry so no need to add prefix for each attempt.
            return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=False,
                timeout=timeout,
            )
        else:
            start_daemon = rank == 0
>           return TCPStore(
                host_name=hostname,
                port=port,
                world_size=world_size,
                is_master=start_daemon,
                timeout=timeout,
                multi_tenant=True,
                use_libuv=use_libuv,
            )
E           torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 8889, useIpv6: false, code: -48, name: EADDRINUSE, message: address already in use

.venv/lib/python3.13/site-packages/torch/distributed/rendezvous.py:198: DistNetworkError
----------------------------- Captured stdout call -----------------------------
Distributed data parallel: gloo master at 127.0.0.1:8889
=========================== short test summary info ============================
FAILED tests/test_atomicdescriptors.py::pytest_atomicdescriptors - assert 1 == 0
FAILED tests/test_examples.py::pytest_examples_energy[qm9-SAGE-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-GIN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-GAT-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-MFC-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-PNA-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-PNAPlus-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-SchNet-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-DimeNet-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-EGNN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-PNAEq-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[qm9-PAINN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-SAGE-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-GIN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-GAT-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-MFC-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-PNA-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-PNAPlus-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-SchNet-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-DimeNet-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-EGNN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-PNAEq-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_energy[md17-PAINN-multihead-GPS]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PNAPlus]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-SchNet]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-DimeNet]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-EGNN]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PNAEq]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-PAINN]
FAILED tests/test_examples.py::pytest_examples_grad_forces[LennardJones-MACE]
FAILED tests/test_forces_equivariant.py::pytest_examples[SchNet-LennardJones]
FAILED tests/test_forces_equivariant.py::pytest_examples[EGNN-LennardJones]
FAILED tests/test_forces_equivariant.py::pytest_examples[DimeNet-LennardJones]
FAILED tests/test_forces_equivariant.py::pytest_examples[PAINN-LennardJones]
FAILED tests/test_forces_equivariant.py::pytest_examples[PNAPlus-LennardJones]
FAILED tests/test_forces_equivariant.py::pytest_examples[MACE-LennardJones]
FAILED tests/test_graphs.py::pytest_train_model[ci.json-SAGE] - torch.distrib...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-GIN] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-GAT] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-MFC] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-PNA] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-PNAPlus] - torch.dist...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-CGCNN] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-SchNet] - torch.distr...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-DimeNet] - torch.dist...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-EGNN] - torch.distrib...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-PNAEq] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-PAINN] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model[ci.json-MACE] - torch.distrib...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-SAGE] - tor...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-GIN] - torc...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-GAT] - torc...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-MFC] - torc...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNA] - torc...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAPlus] - ...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-CGCNN] - to...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-SchNet] - t...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-DimeNet] - ...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-EGNN] - tor...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-PNAEq] - to...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-PAINN] - to...
FAILED tests/test_graphs.py::pytest_train_model[ci_multihead.json-MACE] - tor...
FAILED tests/test_graphs.py::pytest_train_model_lengths[GAT] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model_lengths[PNA] - torch.distribu...
FAILED tests/test_graphs.py::pytest_train_model_lengths[PNAPlus] - torch.dist...
FAILED tests/test_graphs.py::pytest_train_model_lengths[CGCNN] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_lengths[SchNet] - torch.distr...
FAILED tests/test_graphs.py::pytest_train_model_lengths[DimeNet] - torch.dist...
FAILED tests/test_graphs.py::pytest_train_model_lengths[EGNN] - torch.distrib...
FAILED tests/test_graphs.py::pytest_train_model_lengths[PNAEq] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_lengths[PAINN] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[GAT-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNA-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAPlus-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[CGCNN-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[SchNet-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[DimeNet-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[EGNN-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[PNAEq-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_model_lengths_global_attention[PAINN-multihead-GPS]
FAILED tests/test_graphs.py::pytest_train_mace_model_lengths[MACE] - torch.di...
FAILED tests/test_graphs.py::pytest_train_equivariant_model[EGNN] - torch.dis...
FAILED tests/test_graphs.py::pytest_train_equivariant_model[SchNet] - torch.d...
FAILED tests/test_graphs.py::pytest_train_equivariant_model[PNAEq] - torch.di...
FAILED tests/test_graphs.py::pytest_train_equivariant_model[PAINN] - torch.di...
FAILED tests/test_graphs.py::pytest_train_equivariant_model[MACE] - torch.dis...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[GAT] - torch.dis...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[PNA] - torch.dis...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[PNAPlus] - torch...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[SchNet] - torch....
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[DimeNet] - torch...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[EGNN] - torch.di...
FAILED tests/test_graphs.py::pytest_train_model_vectoroutput[PNAEq] - torch.d...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[SAGE] - torch.distr...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[GIN] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[GAT] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[MFC] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNA] - torch.distri...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNAPlus] - torch.di...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[SchNet] - torch.dis...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[DimeNet] - torch.di...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[EGNN] - torch.distr...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PNAEq] - torch.dist...
FAILED tests/test_graphs.py::pytest_train_model_conv_head[PAINN] - torch.dist...
FAILED tests/test_loss_and_activation_functions.py::pytest_loss_functions[mse]
FAILED tests/test_loss_and_activation_functions.py::pytest_loss_functions[mae]
FAILED tests/test_loss_and_activation_functions.py::pytest_loss_functions[rmse]
FAILED tests/test_loss_and_activation_functions.py::pytest_loss_functions[GaussianNLLLoss]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[relu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[selu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[prelu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[elu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_01]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_025]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_multihead[lrelu_05]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[relu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[selu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[prelu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[elu]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_01]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_025]
FAILED tests/test_loss_and_activation_functions.py::pytest_activation_functions_vectoroutput[lrelu_05]
FAILED tests/test_model_loadpred.py::pytest_model_loadpred - torch.distribute...
FAILED tests/test_optimizer.py::pytest_optimizers[False-SGD] - torch.distribu...
FAILED tests/test_optimizer.py::pytest_optimizers[False-Adam] - torch.distrib...
FAILED tests/test_optimizer.py::pytest_optimizers[False-Adadelta] - torch.dis...
FAILED tests/test_optimizer.py::pytest_optimizers[False-Adagrad] - torch.dist...
FAILED tests/test_optimizer.py::pytest_optimizers[False-Adamax] - torch.distr...
FAILED tests/test_optimizer.py::pytest_optimizers[False-AdamW] - torch.distri...
FAILED tests/test_optimizer.py::pytest_optimizers[False-RMSprop] - torch.dist...
FAILED tests/test_optimizer.py::pytest_optimizers[True-SGD] - torch.distribut...
FAILED tests/test_optimizer.py::pytest_optimizers[True-Adam] - torch.distribu...
FAILED tests/test_optimizer.py::pytest_optimizers[True-Adadelta] - torch.dist...
FAILED tests/test_optimizer.py::pytest_optimizers[True-Adagrad] - torch.distr...
FAILED tests/test_optimizer.py::pytest_optimizers[True-Adamax] - torch.distri...
FAILED tests/test_optimizer.py::pytest_optimizers[True-AdamW] - torch.distrib...
FAILED tests/test_optimizer.py::pytest_optimizers[True-RMSprop] - torch.distr...
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[None-bessel-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[None-gaussian-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[None-chebyshev-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-bessel-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-gaussian-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Agnesi-chebyshev-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-bessel-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-gaussian-MACE]
FAILED tests/test_radial_transforms.py::pytest_train_model_transforms[Soft-chebyshev-MACE]
============= 146 failed, 23 passed, 3 skipped in 64.63s (0:01:04) =============
